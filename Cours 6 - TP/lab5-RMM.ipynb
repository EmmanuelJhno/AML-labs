{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Robust Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab session is to study robust mixture models. You will code the EM algorithm to estimate the parameters of a mixture of multivariate t-distributions. \n",
    "\n",
    "You have to send the filled notebook named **\"L5_familyname1_familyname2.ipynb\"** (groups of 2) by email to aml.centralesupelec.2019@gmail.com before October 31 at 23:59. Please put **\"AML-L5\"** in the subject. \n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $t$MM\n",
    "\n",
    "Reference: https://people.smp.uq.edu.au/GeoffMcLachlan/pm_sc00.pdf \n",
    "\n",
    "1 - Prove that in the $t$MM model\n",
    "\n",
    "$$U_i|(X_i=x_i,Z_{ij}=1) \\sim \\Gamma\\left(\\frac{\\nu_j+p}{2}, \\frac{\\nu_j+(x_i-\\mu_j)^T\\Sigma_j^{-1}(x_i-\\mu_j)}{2}\\right)$$\n",
    "\n",
    "2 - Fill in the following class to implement EM for a multivariate $t$MM. You can use the gamma and digamma functions and also a solver to find roots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_tMM():\n",
    "    \n",
    "    def __init__(self, K):\n",
    "        '''\n",
    "        Parameters:\n",
    "        -----------\n",
    "        K: integer\n",
    "            number of components\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        alpha_: np.array\n",
    "            proportion of components\n",
    "        mu_: np.array\n",
    "            array containing means\n",
    "        Sigma_: np.array\n",
    "            array cointaining covariance matrix\n",
    "        cond_prob_: (n, K) np.array\n",
    "            conditional probabilities for all data points \n",
    "        u_: (n, K) np.array\n",
    "            expectation of the U variable \n",
    "        dof_: (K, ) np.array\n",
    "            degrees of freedom of each component\n",
    "        labels_: (n, ) np.array\n",
    "            labels for data points\n",
    "        '''\n",
    "        \n",
    "        self.K = K\n",
    "        self.alpha_ = None\n",
    "        self.mu_ = None\n",
    "        self.Sigma_ = None\n",
    "        self.cond_prob_ = None\n",
    "        self.labels_ = None\n",
    "        self.dof_ = None\n",
    "        self.u_ = None\n",
    "        \n",
    "    def sqr_mahalanobis(x, mu, Sigma):\n",
    "        # compute the squared Mahalanobis distance\n",
    "        return (x - mu).T @ np.linalg.pinv(Sigma) @ (x - mu)\n",
    "    \n",
    "    def student_pdf(self, x_i, mu, Sigma, dof):\n",
    "        \"\"\"\n",
    "        Compute the PDF of a multivariate Student's t-distribution of parameters mu, sigma and dof\n",
    "        evaluated in point x_i.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_i: (p, ) np.array\n",
    "            Point of evaluation\n",
    "        \n",
    "        mu: (p, ) np.array\n",
    "            Estimator of the mean\n",
    "            \n",
    "        sigma: (p, p) np.array\n",
    "            Variance matrix estimator\n",
    "        \n",
    "        dof: \n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        Evaluation of the PDF\n",
    "        \"\"\"\n",
    "        from scipy.special import gamma\n",
    "        \n",
    "        p = x_i.shape[0]\n",
    "        \n",
    "        num = gamma((dof + p)/2)*1/np.sqrt(np.det(Sigma))\n",
    "        den = (np.sqrt(np.pi*dof)**p * gamma(dof/2) * \n",
    "               np.sqrt(1 + self.sqr_mahalanobis(x_i, mu, Sigma)/dof)**(dof+p))\n",
    "        \n",
    "        return num/den\n",
    "    \n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\" Find the parameters\n",
    "        that better fit the data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            Data matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        self\n",
    "        \"\"\"\n",
    "        \n",
    "        self.has_converged = False\n",
    "        \n",
    "        # Initialize parameters estimators\n",
    "        self.mu_, self.sigma_, self.dof_, self.u_ = self.init_params(X, self.K)\n",
    "        \n",
    "        itr = 0\n",
    "        while not self.has_converged and itr < 5:\n",
    "            itr += 1\n",
    "            # E-step: update cond_prob_ and u_\n",
    "            new_cond_prob = self.compute_proba(X)\n",
    "            self.alpha_ = np.sum(cpm, axis=0) / n\n",
    "            new_u = self.compute_u(mu=new_mu, sigma=self.sigma_, dof=self.dof_)\n",
    "            \n",
    "            # M-step: update parameters estimators\n",
    "            #      - mu and sigma values for maximizing the likelihood\n",
    "            #      - the degrees of freedom variable\n",
    "            new_mu = self.update_mu(X=X, cpm=self.cond_prob_)\n",
    "            new_sigma = self.update_sigma(X=X, cpm=self.cond_prob_, mu=new_mu)\n",
    "            new_dof = self.update_dof(X=X, cpm=self.cond_prob_, mu=new_mu, sigma=new_sigma)\n",
    "            \n",
    "            old_params = (self.cond_prob_, self.mu_, self.sigma_, self.dof_, self.u_)\n",
    "            new_params = (new_cond_prob, new_mu, new_sigma, new_dof, new_u)\n",
    "            \n",
    "            self.cond_prob_ = new_cond_prob\n",
    "            self.mu_ = new_mu\n",
    "            self.sigma_ = new_sigma\n",
    "            self.dof_ = new_dof\n",
    "            self.u_ = new_u\n",
    "            \n",
    "            self.has_converged = self.evaluate_convergence(X, old_params=old_params, new_params=new_params)\n",
    "        \n",
    "        # Update labels\n",
    "        self.labels_ = self.predict(X)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def evaluate_convergence(self, X, old_params, new_params):\n",
    "        \"\"\"Returns wether the algorithm has converged\n",
    "        \"\"\"\n",
    "        return abs(self.log_likelihood(X, *old_params) - self.log_likelihood(X, *new_params)) <= self.tol_\n",
    "        \n",
    "    def compute_proba(self, X):\n",
    "        \"\"\" Compute probability vector for X\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            New data matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        proba: (n, k) np.array        \n",
    "        \"\"\"\n",
    "        mu = self.mu_\n",
    "        sigma = self.sigma_\n",
    "        alpha = self.alpha_\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        k = self.k_\n",
    "\n",
    "        prob_matrix = np.zeros(shape=(n, k))\n",
    "\n",
    "        for i in range(n):\n",
    "            joint_law = 1e-3  # To prevent division by 0\n",
    "            x_i = X[i]\n",
    "            for j in range(k):\n",
    "                prob_matrix[i, j] = alpha[j] * self.student_pdf(x_i=x_i, mu=mu[j], sigma=sigma[j])\n",
    "                joint_law += prob_matrix[i, j]\n",
    "            prob_matrix[i, :] /= joint_law\n",
    "        \n",
    "        return prob_matrix\n",
    "    \n",
    "    def init_params(self, X, K):\n",
    "        \"\"\"Initialize parameters estimators\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            New data matrix\n",
    "        \n",
    "        K: int\n",
    "            Number of classes / clusters\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        ((k,) np.array, (k, p) np.array, (k, p, p) np.array) tuple\n",
    "            Tuple of 3 np.arrays: (alpha0, mu0, sigma0)\n",
    "        \"\"\"        \n",
    "        n = X.shape[0]\n",
    "        p = X.shape[1]\n",
    "        \n",
    "        mu0 = np.zeros(shape=(K, p))\n",
    "        sigma0 = np.zeros(shape=(K, p, p))\n",
    "        dof0 = np.zeros(shape=(K, p, p, p))\n",
    "        \n",
    "        if self.init_strat_ == \"k_means\":\n",
    "            from sklearn.cluster import KMeans\n",
    "            k_means = KMeans(n_clusters=K).fit(X)\n",
    "            X_labels = k_means.labels_\n",
    "            \n",
    "            cond_prob0 = np.zeros(shape=(n, K))\n",
    "            \n",
    "            for i in range(n):\n",
    "                cond_prob0[i, X_labels[i]] = 1\n",
    "            \n",
    "            mu0 = self.update_mu(cpm=cond_prob0, X=X)\n",
    "            sigma0 = self.update_sigma(cpm=cond_prob0, X=X, mu=mu0)\n",
    "            dof0 = self.update_dof(cpm=cond_prob0, p=X.shape[1], mu=mu0, sigma=sigma0)\n",
    "            u0 = self.update_u(X=X, cpm=cond_prob0, mu=mu0, sigma=sigma0)\n",
    "        \n",
    "        else:\n",
    "            # Uniform initilization\n",
    "            cond_prob0 = (1/K) * np.ones(shape=(n, K))\n",
    "            \n",
    "            mu0 = np.random.rand(K,p)\n",
    "            sigma0 = np.array([np.eye(p,p)] * K)\n",
    "            dof0 = np.array([np.eye(p,p,p)] * K)\n",
    "            \n",
    "        return (mu0, sigma0, dof0, u0)\n",
    "    \n",
    "    def update_mu(self, X, cpm):\n",
    "        \"\"\"Returns the updated version of mu (cluster mean estimators)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            Data\n",
    "        \n",
    "        cpm: (n, k) np.array\n",
    "            Conditional probability matrix\n",
    "        \n",
    "        alpha: (k, ) np.array\n",
    "            Current estimators for the clusters' proportions\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        (k, p) np.array\n",
    "            The new estimators for clusters' means\n",
    "        \"\"\"\n",
    "        n = cpm.shape[0]\n",
    "        k = cpm.shape[1]   \n",
    "        p = X.shape[1]\n",
    "\n",
    "        new_mu = np.zeros(shape=(k, p))\n",
    "\n",
    "        for j in range(k):\n",
    "            for i in range(n):\n",
    "                new_mu[j] += X[i] * cpm[i, j] * u[i, j]\n",
    "            new_mu[j] /= np.sum(np.multiply(cpm,u)[j])\n",
    "\n",
    "        return new_mu\n",
    "\n",
    "    def update_sigma(self, X, cpm, mu):\n",
    "        \"\"\"Returns the updated version of sigma (cluster mean covariances)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            Data\n",
    "        \n",
    "        cpm: (n, k) np.array\n",
    "            Conditional probability matrix\n",
    "        \n",
    "        alpha: (k, ) np.array\n",
    "            Current estimators for the clusters' proportions\n",
    "        \n",
    "        mu: (k, p) np.array\n",
    "            Current estimators for the clusters' means\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        (k, p, p) np.array\n",
    "            The new estimators for clusters' variances\n",
    "        \"\"\"\n",
    "        n = cpm.shape[0]\n",
    "        k = cpm.shape[1]\n",
    "        p = X.shape[1]\n",
    "        \n",
    "        new_sigma = np.zeros(shape=(k, p, p))\n",
    "        \n",
    "        for j in range(k):\n",
    "            for i in range(n):\n",
    "                deviation = X[i] - mu[j]\n",
    "                new_sigma[j] += deviation @ deviation.T * cpm[i, j] * u[i, j]\n",
    "            new_sigma[j] /= np.sum(cpm[j])\n",
    "        \n",
    "        return new_sigma\n",
    "    \n",
    "    def function_dof(dof, old_dof, cond_prob, u, k, p):\n",
    "        # function to find dof\n",
    "        from scipy.specials import digamma as psi\n",
    "        \n",
    "        num = cond_prob[:,k].T @ (np.ln(u) - u)[:,k]\n",
    "        den = cond_prob[:,k].T @ np.ones((p,1))\n",
    "        \n",
    "        return -psi(dof/2) + np.log(dof/2) + 1 + psi((old_dof+p)/2) - np.log((old_dof+p)/2) + nom/den\n",
    "        \n",
    "    \n",
    "    def update_dof(self, cpm, p, old_dof, u):\n",
    "        \"\"\"Returns the updated version of dof (cluster mean degree of liberty)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        cpm: (n, k) np.array\n",
    "            Conditional probability matrix\n",
    "            \n",
    "        p: int\n",
    "            The dimension of the data\n",
    "        \n",
    "        old_dof: (k, ) np.array\n",
    "            Old estimators for the clusters' degree of freedom\n",
    "        \n",
    "        u: (n, k) np.array\n",
    "            Current estimators for the clusters' latent variable u\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        (k, ) np.array\n",
    "            New estimators for clusters' degree of freedom\n",
    "        \"\"\"\n",
    "        from scipy.optimize import root_scalar\n",
    "        \n",
    "        n = cpm.shape[0]\n",
    "        k = cpm.shape[1]\n",
    "        \n",
    "        new_dof = np.zeros(shape=(k, ))\n",
    "        \n",
    "        for j in range(k):\n",
    "            new_dof[j] = root_scalar(function_dof(args=old_dof[j], cpm, u, k, p)).root\n",
    "        \n",
    "        return new_dof\n",
    "    \n",
    "    def update_u(self, X, cpm, mu, sigma):\n",
    "        \"\"\"Returns the updated version of u\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            Data\n",
    "        \n",
    "        cpm: (n, k) np.array\n",
    "            Conditional probability matrix\n",
    "        \n",
    "        alpha: (k, ) np.array\n",
    "            Current estimators for the clusters' proportions\n",
    "        \n",
    "        mu: (k, p) np.array\n",
    "            Current estimators for the clusters' means\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        (n, k) np.array\n",
    "            The new u\n",
    "        \"\"\"\n",
    "        n = cpm.shape[0]\n",
    "        k = cpm.shape[1]\n",
    "        p = X.shape[1]\n",
    "        \n",
    "        new_u = np.zeros(shape=(n, k))\n",
    "        \n",
    "        for j in range(k):\n",
    "            for i in range(n):\n",
    "                num = dof[j]+p\n",
    "                den = dof[j] + (x[i]-mu[j]).T @ np.linalg.pinv(Sigma)[j] @ (x[i]-mu[j])\n",
    "                new_u[i,j] = num/den\n",
    "        \n",
    "        return new_u\n",
    "    \n",
    "    def gauss_pdf(self, x_i, mu, sigma):\n",
    "        \"\"\"\n",
    "        Computes the PDF of a Gaussian of parameters sigma and mu,\n",
    "        evaluated in point x_i\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_i: (p, ) np.array\n",
    "            Point of evaluation\n",
    "        \n",
    "        mu: (p, ) np.array\n",
    "            Estimator of the mean\n",
    "            \n",
    "        sigma: (p, p) np.array\n",
    "            Variance matrix estimator\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        Evaluation of the PFD\n",
    "        \"\"\"\n",
    "        C = 1/(np.sqrt(2 * np.pi * np.linalg.det(sigma)))\n",
    "        sigma_inv = np.linalg.pinv(sigma)\n",
    "        #print(C, sigma, np.linalg.det(sigma))\n",
    "        exp = np.exp(-0.5 * np.transpose(x_i - mu).dot(sigma_inv).dot(x_i - mu))\n",
    "        return C * exp\n",
    "    \n",
    "    def gamma_pdf(self, x, lbda, theta):\n",
    "        \"\"\"\n",
    "        Computes the PDF of a Gamma distribution of parameters sigma and mu,\n",
    "        evaluated in point x_i\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_i: (p, ) np.array\n",
    "            Point of evaluation\n",
    "        \n",
    "        mu: (p, ) np.array\n",
    "            Estimator of the mean\n",
    "            \n",
    "        sigma: (p, p) np.array\n",
    "            Variance matrix estimator\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        Evaluation of the PDF\n",
    "        \"\"\"\n",
    "        from scipy.special import gamma as GAMMA\n",
    "        \n",
    "        num = x**(lbda-1) * np.exp(-x/theta)\n",
    "        den = GAMMA(lbda)*theta**lbda\n",
    "        \n",
    "        return num/den\n",
    "    \n",
    "    def log_likelihood(self, X, cmp, mu, sigma, dof):\n",
    "        \"\"\"Returns whether the algorithm has converged\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            Data\n",
    "        \n",
    "        alpha: (k, ) np.array\n",
    "            Current estimators for the clusters' proportions\n",
    "        \n",
    "        mu: (k, p) np.array\n",
    "            Current estimators for the clusters' means\n",
    "            \n",
    "        sigma: (k, p, p) np.array\n",
    "            Current estimators for clusters' variances\n",
    "            \n",
    "        dof: (k, ) np.array\n",
    "            Current estimators for clusters' degree of freedom\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        float\n",
    "            log-likelihood for the given parameters\n",
    "        \"\"\"\n",
    "        l = 0\n",
    "        n = X.shape[0]\n",
    "        k = alpha.shape[0]\n",
    "        \n",
    "        for i in range(n):\n",
    "            x_i = X[i]\n",
    "            for j in range(k):\n",
    "                if self.labels_[i]==j:\n",
    "                    PDF_1 = gauss_pfd(x_i, mu=mu[j], sigma=sigma[j])\n",
    "\n",
    "                    lbda = (dof[j]+p)/2\n",
    "                    theta = 0.5 * (x[i]-mu[j]).T @ np.linalg.pinv(Sigma)[j] @ (x[i]-mu[j])\n",
    "                    PDF_2 = self.gamma_pdf(x_i, lbda, theta)\n",
    "                    PDF_3 = cmp[i,j]\n",
    "                    l += np.log(PDF_1*PDF_2*PDF_3)\n",
    "\n",
    "        return l\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict labels for X\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            New data matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        label assigment        \n",
    "        \"\"\" \n",
    "        n= X.shape[0]\n",
    "        labels = np.zeros(shape=(n,))\n",
    "        cond_prob_mat = self.compute_proba(X)\n",
    "        \n",
    "        for i in range(n):\n",
    "            if self.thresholding_strategy_ == \"soft\":\n",
    "                labels[i] = np.argmax(np.random.multinomial(1, cond_prob_mat[i, :]))\n",
    "            else:\n",
    "                labels[i] = np.argmax(cond_prob_mat[i, :])\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Generate one dataset with mixtures of t-distributions that ilustrate when tMM and GMM behave similarly and another dataset where tMM has a better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Modify the my_GMM class to implement the Extra Uniform Cluster Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Modify the my_GMM class to implement the trimmed EM for GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 - Compare the 4 methods in one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS (not graded):** Implement the trimming EM clustering algorithm TCLUST (https://arxiv.org/pdf/0806.2976.pdf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
