{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Graphical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab session is to code two methods to estimate the structure of undirected gaussian graphical models and compare them.\n",
    "\n",
    "You have to send the filled notebook named **\"L8_familyname1_familyname2.ipynb\"** (groups of 2) by email to aml.centralesupelec.2019@gmail.com before December 12, 2019 at 23:59 and put **\"AML-L8\"** as subject. \n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graphical model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. The variables are represented by nodes and the relations between them are represented by edges.\n",
    "\n",
    "### GLasso\n",
    "\n",
    "Graphical Lasso is the name of the optimization problem that estimates the precision matrix of a multivariate gaussian and its name comes from the direct link with graphical models and the regularization term. \n",
    "\n",
    "Fill in the following class that implements the GLasso algorithm optimized by ADMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_GLasso():\n",
    "    \n",
    "    def __init__(self, alpha, mu, max_iter = 60, tol=1e-4):\n",
    "        '''\n",
    "        Parameters:\n",
    "        alpha : float\n",
    "            Penalization parameter selected.\n",
    "        mu: float>0\n",
    "\n",
    "        Attributes:\n",
    "        \n",
    "        covariance_ : numpy.ndarray, shape (n_features, n_features)\n",
    "            Estimated covariance matrix.\n",
    "        precision_ : numpy.ndarray, shape (n_features, n_features)\n",
    "            Estimated precision matrix (inverse covariance).\n",
    "        '''\n",
    "        self.covariance_ = None\n",
    "        self.precision_ = None\n",
    "        self.alpha_ = alpha\n",
    "        self.mu_ = mu\n",
    "        self.iter_max_ = max_iter\n",
    "        self.tol_ = tol\n",
    "        \n",
    "        self.history_ = None\n",
    "        \n",
    "    def update_precision_(self, S, Z, V, mu):\n",
    "        \"\"\"\n",
    "        Performs an update of the precision matrix P\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "        S: (n_features, n_features) numpy.ndarray\n",
    "            Empirical covariance matrix\n",
    "        \n",
    "        Z: (n_features, n_features) numpy.ndarray\n",
    "            Dual variable for precision matrix\n",
    "        \n",
    "        V: (n_features, n_features) numpy.ndarray\n",
    "            Duality jump\n",
    "        \n",
    "        mu: float > 0\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        P: (n_features, n_features) numpy.ndarray\n",
    "            The precision matrix minimizing the gradient of Lagragian wrt P\n",
    "        \"\"\"\n",
    "        T = Z - V - (1/mu)*S\n",
    "        eig_values, eig_vectors = np.linalg.eig(T)\n",
    "        new_eig_values = (eig_values + np.sqrt(eig_values**2 + 4/mu))/2\n",
    "        return eig_vectors.T @ np.diag(new_eig_values) @ eig_vectors\n",
    "    \n",
    "    def update_dual_(self, P, S, V, alpha, mu):\n",
    "        \"\"\"\n",
    "        Performs an update of the precision matrix dual Z\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "        S: (n_features, n_features) numpy.ndarray\n",
    "            Empirical covariance matrix\n",
    "        \n",
    "        P: (n_features, n_features) numpy.ndarray\n",
    "            Precision matrix estimate\n",
    "        \n",
    "        V: (n_features, n_features) numpy.ndarray\n",
    "            Duality jump\n",
    "        \n",
    "        alpha: float > 0\n",
    "            Regularization parameter\n",
    "        \n",
    "        mu: float > 0\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Z: (n_features, n_features) numpy.ndarray\n",
    "            The dual precision matrix minimizing the gradient of Lagragian wrt Z\n",
    "        \"\"\"\n",
    "        return np.multiply(np.sign(P+V), np.maximum(0, np.absolute(P + V) - alpha/mu))\n",
    "    \n",
    "    def update_jump_(self, old_V, P, Z, mu):\n",
    "        \"\"\"\n",
    "        Updates the duality jump V\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "        old_V: (n_features, n_features) numpy.ndarray\n",
    "            Old duality jump\n",
    "        \n",
    "        P: (n_features, n_features) numpy.ndarray\n",
    "            Precision matrix estimate\n",
    "            \n",
    "        Z: (n_features, n_features) numpy.ndarray\n",
    "            Dual variable for precision matrix\n",
    "        \n",
    "        mu: float > 0\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        (n_features, n_features) numpy.ndarray\n",
    "            New value for the duality jump\n",
    "        \"\"\"\n",
    "        return old_V + mu*(P-Z)\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\" Fits the GraphicalLasso model to X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            Data matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        self\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Compute empirical estimators for esperance and covariance\n",
    "        esperance = np.mean(X, axis=0)\n",
    "        \n",
    "        # TODO - optimize this calculation\n",
    "        S = 0\n",
    "        for i in range(n_samples):\n",
    "            dev = (X[i] - esperance).reshape(-1,1)\n",
    "            S += dev@dev.T\n",
    "        S /= n_samples\n",
    "        \n",
    "        # Initialize latent variable to definite positive symetric matrices\n",
    "        Zn = np.eye(n_features)\n",
    "        Vn = np.eye(n_features)\n",
    "        Pn = np.eye(n_features)\n",
    "        \n",
    "        self.history_ = {\n",
    "            \"precision\": [Pn],\n",
    "            \"dual\": [Zn],\n",
    "            \"jump\": [Vn],\n",
    "        }\n",
    "                \n",
    "        converged = False\n",
    "        itr = 0\n",
    "        \n",
    "        while itr < self.iter_max_ and not converged:\n",
    "            \n",
    "            Pnp1 = self.update_precision_(S, Zn, Vn, self.mu_)\n",
    "            Znp1 = self.update_dual_(Pnp1, S, Vn, self.alpha_, self.mu_)\n",
    "            Vnp1 = self.update_jump_(Vn, Pnp1, Znp1, self.mu_)\n",
    "            \n",
    "            converged = np.linalg.norm(Pn - Pnp1) < self.tol_ * np.linalg.norm(Pn)\n",
    "        \n",
    "            Pn = Pnp1\n",
    "            Vn = Vnp1\n",
    "            Zn = Znp1\n",
    "            \n",
    "            self.history_[\"precision\"].append(Pn)\n",
    "            self.history_[\"dual\"].append(Zn)\n",
    "            self.history_[\"jump\"].append(Vn)\n",
    "            itr += 1\n",
    "        \n",
    "        self.precision_ = Zn\n",
    "        self.covariance_ = np.linalg.inv(Zn)\n",
    "        return self        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodewise Regression\n",
    "\n",
    "Fill in the following class that implements the nodewise regression algorithm to estimate a graphical model structure. You can use `LassoCV` for the regressions. Bonus (not graded): Implement your own cross-validation lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "class my_nodewise_regression():\n",
    "    \n",
    "    def __init__(self, rule, alpha):\n",
    "        '''\n",
    "        Parameters:\n",
    "        \n",
    "        rule: {\"OR\", \"AND\"}\n",
    "        alpha: float\n",
    "            regularization parameter\n",
    "        \n",
    "        Attributes:\n",
    "        \n",
    "        covariance_structure_ : numpy.ndarray, shape (n_features, n_features)\n",
    "            Estimated covariance matrix.        \n",
    "        '''\n",
    "        self.graph_structure_ = None\n",
    "        self.rule_ = rule\n",
    "        self.alpha_ = alpha\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\" Fit the model to X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            Data matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        self\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        beta = np.zeros((n_features, n_features))\n",
    "        for j in range(n_features):\n",
    "            y = X[j]#.reshape(-1,1)\n",
    "            x = np.delete(X, j, axis=0)\n",
    "            regressor = sklearn.linear_model.Lasso(alpha=self.alpha_)\n",
    "            regressor.fit(X=x, y=y)\n",
    "            beta[j, :j], beta[j, j+1:] = regressor.coef_[:j], regressor.coef_[j:]\n",
    "        \n",
    "        domain_estimate = beta > 1e-7\n",
    "        \n",
    "        if self.rule_ == \"OR\":\n",
    "            adj = beta | beta.T\n",
    "        \n",
    "        elif self.rule == \"AND\":\n",
    "            adj = beta & beta.T\n",
    "        \n",
    "        self.graph_structure_ = adj\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an easy-to-check (non-trivial, p<=6) example and plot the 4 (real, GLasso, AND, OR) graphs. You can use `networkx` to plot the resulting graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu=[0., 0., 0.]\n",
    "cov = np.array([\n",
    "    [1., 0., 1.],\n",
    "    [0., 1., 0.],\n",
    "    [1., 0., 1.]\n",
    "])\n",
    "cov = cov.T@cov\n",
    "\n",
    "X = np.array([np.random.multivariate_normal(mu, cov) for _ in range(100)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_GLasso(alpha=0.1, mu=1)\n",
    "\n",
    "model.fit(X)\n",
    "\n",
    "G = nx.from_numpy_array(model.covariance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.24612866, -0.79599848, -0.0751022 ],\n",
       "       [-0.79599848,  1.6979309 , -0.02150432],\n",
       "       [-0.0751022 , -0.02150432,  0.58288455]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.covariance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations\n",
    "\n",
    "Compare the two graph estimators for the following model with $p = 300$ and $n = 40, 80, 320$:\n",
    "\n",
    "- An AR(1)-Block model. In this model the *covariance* matrix is block-diagonal with equalsized AR(1)-blocks of the form $(\\Sigma_{Block})_{i, j} = 0.9^{|i−j|}$, take $30 \\times 30$ blocks.\n",
    "\n",
    "Report accuracy and F1 score for the edge estimation.\n",
    "\n",
    "For GLasso estimation, use cross-validation k-fold with loglikelihood loss to select the $\\lambda$ penalization parameter.\n",
    "\n",
    "Measure the estimation error of the GLasso matrix result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
